{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve Gridworld with policy iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from Gridworld import Gridworld\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function returns current position of player as coordinates\n",
    "def find_position_player(array_playing_field):\n",
    "    player = array_playing_field[0]\n",
    "    for i in range(len(player)):\n",
    "        if max(player[i])==1:\n",
    "            x = i\n",
    "    for j in range(len(player)):\n",
    "        if player[x][j]==1:\n",
    "            y = j\n",
    "    return (x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function returns the instant reward given a position\n",
    "def instant_reward(array, position):\n",
    "    wall = array[3]\n",
    "    pit = array[2]\n",
    "    goal = array[1]\n",
    "\n",
    "    first = position[0]\n",
    "    second = position[1]\n",
    "\n",
    "    if goal[first, second] == 1:\n",
    "        return 10\n",
    "    elif wall[first, second] == 1:\n",
    "        return -50\n",
    "    elif pit[first, second] == 1:\n",
    "        return -100\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function calculates the reward given a position and a move\n",
    "def return_reward (array, position, move, grid):\n",
    "    wall = array[3]\n",
    "    pit = array[2]\n",
    "    goal = array[1]\n",
    "\n",
    "    if (position[0]== 0) & (move =='u'):\n",
    "        return (-1, position[grid -1], position[1])\n",
    "    elif (position[1]== 0) & (move =='l'):\n",
    "        return (-1, position[0], position[1])\n",
    "    elif (position[0]== 3) & (move =='d'):\n",
    "        return (-1, position[0], position[1])\n",
    "    elif (position[1]== grid -1) & (move =='r'):\n",
    "        return (-1, position[0], position[1])\n",
    "    else:\n",
    "        first = position[0]\n",
    "        second = position[1]\n",
    "        if move == 'u':\n",
    "            first = position[0] - 1\n",
    "        elif move == 'd':\n",
    "            first =  position[0] + 1\n",
    "        elif move == 'r':\n",
    "            second = position[1] + 1\n",
    "        elif move == 'l':\n",
    "            second = position[1] - 1\n",
    "        else:\n",
    "            return 'error'\n",
    "        \n",
    "        inst_reward = instant_reward(array,(first, second)) \n",
    "\n",
    "        return (inst_reward, first, second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function return the value from q-mat based on position and move\n",
    "def get_q_value (q_mat, position, move, grid):\n",
    "  \n",
    "    # if move would take player off the board\n",
    "    if (position[0]== 0) & (move =='u'):\n",
    "        return q_mat[position[0], position[1]]\n",
    "    elif (position[1]== 0) & (move =='l'):\n",
    "        return q_mat[position[0], position[1]]\n",
    "    elif (position[0]== grid -1) & (move =='d'):\n",
    "        return q_mat[position[0], position[1]]\n",
    "    elif (position[1]== grid -1) & (move =='r'):\n",
    "        return q_mat[position[0], position[1]]\n",
    "    \n",
    "    # actual move\n",
    "    else:\n",
    "        first = position[0]\n",
    "        second = position[1]\n",
    "        if move == 'u':\n",
    "            first = position[0] - 1\n",
    "        elif move == 'd':\n",
    "            first =  position[0] + 1\n",
    "        elif move == 'r':\n",
    "            second = position[1] + 1\n",
    "        elif move == 'l':\n",
    "            second = position[1] - 1\n",
    "        else:\n",
    "            return 'error'\n",
    "\n",
    "\n",
    "        return q_mat[first, second]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function calculates the best policies from position\n",
    "def update_prob_vector(q_mat, prob_vector, position, grid):\n",
    "    moves = ['u','r','d','l']\n",
    "    store_v_next = []\n",
    "    for move in moves:\n",
    "        store_v_next.append(get_q_value( q_mat, position, move, grid=grid)) # only the first element is the reward\n",
    "    max = np.max(store_v_next)\n",
    "    # seach for max in vector and count how often it appears\n",
    "    prob_vector = np.zeros(grid)\n",
    "    for i in range(len(store_v_next)):\n",
    "        if store_v_next[i] == max:\n",
    "            prob_vector[i] = 1\n",
    "      \n",
    "    \n",
    "    prob_vector /= prob_vector.sum()\n",
    "    return prob_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates the value of being at position based on what is possible in the next state\n",
    "def exp_val_next_state(q_mat, position, prob_vector, grid):\n",
    "    moves = ['u','r','d','l']\n",
    "    expected_value = 0\n",
    "    for i in range(len(moves)):\n",
    "        move = moves[i]\n",
    "        expected_value += prob_vector[i] * get_q_value(q_mat, position, move, grid=grid)\n",
    "    return expected_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mat_policy_vec(dimension, grid):\n",
    "    prob_vector = np.ones(4)\n",
    "    prob_vector /= prob_vector.sum()\n",
    "    prob_vector\n",
    "\n",
    "    # set up a matrix storing for each position the policy vector\n",
    "    mat_of_prob_vec = []\n",
    "    for i in range (grid):\n",
    "        temp = []\n",
    "        for j in range (grid):\n",
    "            temp.append(prob_vector)\n",
    "        mat_of_prob_vec.append(temp)\n",
    "\n",
    "    return mat_of_prob_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' ' ' ' ' ' ' ']\n",
      " [' ' '-' ' ' ' ']\n",
      " ['+' ' ' 'W' ' ']\n",
      " [' ' ' ' 'P' ' ']]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# instantiate a random game and show board\n",
    "grid = 4\n",
    "game = Gridworld(size=grid, mode='random')\n",
    "print(game.display())\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' ' ' ' 'P' ' ']\n",
      " [' ' 'W' ' ' ' ']\n",
      " [' ' ' ' '-' ' ']\n",
      " [' ' ' ' '+' ' ']]\n",
      "\n",
      "position:  (0, 2)\n",
      "position:  (0, 3)\n",
      "position:  (1, 3)\n",
      "position:  (2, 3)\n",
      "position:  (3, 3)\n",
      "game won!\n"
     ]
    }
   ],
   "source": [
    "# instantiate a random game and try to solve it within 10 moves\n",
    "\n",
    "grid = 4\n",
    "game = Gridworld(size=grid, mode='random')\n",
    "print(game.display())\n",
    "print('')\n",
    "\n",
    "# define possible moves\n",
    "moves = ['u','r','d','l']\n",
    "\n",
    "array_playing_field = game.board.render_np()\n",
    "\n",
    "gamma = 0.9 # discount factor\n",
    "q_mat = np.zeros((grid,grid)) # q_mat initialize to zero\n",
    "mat_policy_vec = create_mat_policy_vec(dimension=grid, grid=grid) # matrix with initialized policy vector for each position\n",
    "\n",
    "q_mat_next = np.zeros((grid,grid)) # q_mat for the next state initialized to zero\n",
    "\n",
    "# loops to update q_mat and policies\n",
    "counter = 0\n",
    "while counter < 10:\n",
    "\n",
    "    # update q_mat\n",
    "    for i in range(grid):\n",
    "        for j in range(grid):\n",
    "            # value is sum of instant reward plus expected value\n",
    "            q_mat_next[i][j] = instant_reward(array = array_playing_field, position=(i,j)) + gamma * exp_val_next_state(q_mat=q_mat, position=(i,j), prob_vector=mat_policy_vec[i][j], grid=grid) # Bellman equation\n",
    "\n",
    "    #print(q_mat_next)\n",
    "    # update mat_prob_vec\n",
    "    for i in range(grid):\n",
    "        for j in range(grid):\n",
    "            mat_policy_vec[i][j] = update_prob_vector(q_mat=q_mat_next, prob_vector=mat_policy_vec[i][j], position=(i,j), grid=grid)\n",
    "    \n",
    "    # update q_mat\n",
    "    q_mat = q_mat_next.copy()\n",
    "    counter += 1\n",
    "\n",
    "list_of_moves = []\n",
    "while game.reward()==-1:\n",
    "    position = find_position_player(array_playing_field=game.board.render_np())\n",
    "    print('position: ',position)\n",
    "    move_idx = np.argmax(mat_policy_vec[position[0]][position[1]])\n",
    "    list_of_moves.append(moves[move_idx])\n",
    "    game.makeMove(moves[move_idx])\n",
    "\n",
    "if game.reward() == 10:\n",
    "    print('game won!')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['r', 'd', 'd', 'd', 'l']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the moves\n",
    "list_of_moves"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
